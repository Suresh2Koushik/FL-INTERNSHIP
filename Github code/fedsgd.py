# -*- coding: utf-8 -*-
"""FEDSGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVgYnD96JY6VHcPs0oefdFAcxRBO5rHt
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
import syft as sy
import copy
import numpy as np

import importlib
importlib.import_module('FLDataset')
from FLDataset import load_dataset, getActualImgs
from utils import averageModels, averageGradients

class Arguments():
    def __init__(self):
        self.images = 60000
        self.clients = 10
        self.epochs = 5
        self.local_batches = self.images // self.clients
        self.lr = 0.01
        self.torch_seed = 0
        self.log_interval = 10
        self.iid = 'iid'
        self.split_size = int(self.images / self.clients)
        self.samples = self.split_size / self.images 
        self.use_cuda = False
        self.save_model = False

args = Arguments()

use_cuda = args.use_cuda and torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}

hook = sy.TorchHook(torch)
clients = []

for i in range(args.clients):
    clients.append({'hook': sy.VirtualWorker(hook, id="client{}".format(i+1))})

# Download MNIST manually using 'wget' then uncompress the file
!wget www.di.ens.fr/~lelarge/MNIST.tar.gz
!tar -zxvf MNIST.tar.gz

global_train, global_test, train_group, test_group = load_dataset(args.clients, args.iid)

for inx, client in enumerate(clients):
    trainset_ind_list = list(train_group[inx])
    client['trainset'] = getActualImgs(global_train, trainset_ind_list, args.local_batches)
    client['testset'] = getActualImgs(global_test, list(test_group[inx]), args.local_batches)
    client['samples'] = len(trainset_ind_list) / args.images

global_test_loader = DataLoader(global_test, batch_size=args.local_batches, shuffle=True)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4*4*50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

def train(args, clientss, device, epoch):
    client['model'].train()
    for batch_idx, (data, target) in enumerate(client['trainset']):
        data = data.send(client['hook'])
        target = target.send(client['hook'])
        client['model'].send(data.location)

        data, target = data.to(device), target.to(device)
        client['optim'].zero_grad()
        output = client['model'](data)
        loss = F.nll_loss(output, target)
        loss.backward()
#         client['optim'].step()
        client['model'].get() 

        if batch_idx % args.log_interval == 0:
            loss = loss.get() 
            print('Model {} Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                client['hook'].id,
                epoch, batch_idx * args.local_batches, len(client['trainset']) * args.local_batches, 
                100. * batch_idx / len(client['trainset']), loss.item()))

def test(args, model, device, test_loader, name):
    model.eval()   
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability 
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss for {} model: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        name, test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

class FedSGDOptim(optim.Optimizer):
    def __init__(self, params, lr=args.lr):
        defaults = dict(lr=lr)
        super(FedSGDOptim, self).__init__(params, defaults)

    def step(self, grad_model=None, closure = None):
        loss = None
        if closure is not None:
            loss = closure()
        for group in self.param_groups:
            lr = group['lr']
            for p in zip(group['params'], list(grad_model.parameters())): # (p[0], p[1])
                if p[0].grad is None:
                    continue
    #       d_p = p[0].grad.data # local model grads
                p[0].data.add_(-group['lr'], p[1].grad.data.clone())  
          
        return loss

torch.manual_seed(args.torch_seed)
global_model = Net().to(device)
optimizer = FedSGDOptim(global_model.parameters(), lr=args.lr)
grad_model = Net().to(device)

for client in clients:
    torch.manual_seed(args.torch_seed)
    client['model'] = Net().to(device)
    client['optim'] = optim.SGD(client['model'].parameters(), lr=args.lr)

for epoch in range(1, args.epochs + 1):
    
    for client in clients:
        train(args, client, device, epoch)
    
    grad_model = averageGradients(global_model, clients)
    
#     # Testing 
#     for client in clients:
#         test(args, client['model'], device, client['testset'], client['hook'].id)

    test(args, global_model, device, global_test_loader, 'Global')
    optimizer.step(grad_model)
    test(args, global_model, device, global_test_loader, 'Global')
    
    # Share global model
    for client in clients:
        client['model'].load_state_dict(global_model.state_dict())

if (args.save_model):
    torch.save(global_model.state_dict(), "FedSGD.pt")

list(global_model.parameters())[0].grad

optimizer??

